{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98fe97d4-7e02-4456-af46-c2397c38c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline, StableDiffusionXLPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eefd23a-72fb-4de8-9129-122cc4d01825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef79634058114560a9d11acaa3014b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8c289be98741ab824adeaa85e7c394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "# pipe = StableDiffusion3Pipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9cd69d-e9ac-49be-96df-f9124edd4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32, \n",
    "#     lora_dropout=0.1, \n",
    "#     bias=\"none\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed570931-9fdd-4cbe-89a9-d37b6a1223a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b198bddd19f74b18a9235c5dec9c1e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cc36213e2f44658f5fdfe43e60c84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb9fef93c0e4d438cb1d9e0601da0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc9d6c30dec4f7989e6143b11e73be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6db3b16f95404a9086d804f861f4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f43df9d0f470fb970f4f8598b11bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6252ae7f4aa443cbbe63c28ead53a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6520c730-fa2e-45f1-8aa8-20fc6c65c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    caption = model.generate(**inputs)\n",
    "    return processor.decode(caption[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8634a785-740a-427e-9055-3538071c2c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p03.jpg: gnome with a flower and butterfly on a white background\n",
      "p06.jpg: a black and white drawing of a hand with a triangle and all seeing symbols\n",
      "ghouldfriend_p01.png: a close up of a black bat with big eyes and a big nose\n",
      "p02.jpg: cartoon illustration of a bee with a pot of honey\n",
      "p12.png: two wooden nutcrackers with pine cones and hollyconnets on them\n",
      "p10.jpg: there is a snowman with a hat and scarf holding a present\n",
      "p01.jpg: gnome with a butterfly on his head holding a flower pot\n",
      "p11.jpg: a close up of a toy figure of a cat with a cup\n",
      "p09.jpg: there is a snowman with a scarf and hat holding a red ribbon\n",
      "p08.jpg: a close up of a penguin with a christmas present on a calendar\n",
      "p04.jpg: a close up of a halloween candle holder with a witch hat on top\n",
      "p05.jpg: there are three pumpkins stacked on top of each other\n",
      "p07.jpg: a close up of a skeleton hand holding a skull on a stand\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"datasets/designs/\"\n",
    "caption_file = \"datasets/caption.txt\"\n",
    "for img in os.listdir(image_folder):\n",
    "    caption = generate_caption(os.path.join(image_folder, img))\n",
    "    print(f\"{img}: {caption}\")\n",
    "    with open(caption_file, \"a\") as f:\n",
    "        f.write(f'{img}\\t{caption}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3af273-584f-45b7-9992-48e16a272f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_file, tokenizer_1, tokenizer_2, image_size=1024):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size\n",
    "        self.tokenizer_1 = tokenizer_1\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        \n",
    "        # Load captions\n",
    "        self.image_caption_pairs = []\n",
    "        with open(caption_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_name, caption = line.strip().split('\\t')\n",
    "                self.image_caption_pairs.append((image_name, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_caption_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption = self.image_caption_pairs[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB')\n",
    "        image = image.resize((self.image_size, self.image_size))\n",
    "        image = torch.from_numpy(np.array(image)).float() / 127.5 - 1\n",
    "        image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Tokenize caption with both tokenizers\n",
    "        tokens_1 = self.tokenizer_1(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tokens_2 = self.tokenizer_2(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids_1\": tokens_1.input_ids[0],\n",
    "            \"input_ids_2\": tokens_2.input_ids[0],\n",
    "            \"attention_mask_1\": tokens_1.attention_mask[0],\n",
    "            \"attention_mask_2\": tokens_2.attention_mask[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "066f2d1e-6c62-49cf-8295-5b6255732405",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2320730806.py, line 94)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 94\u001b[0;36m\u001b[0m\n\u001b[0;31m    pooled_prompt_embeds = text_encoder_2(input_ids_2, , output_hidden_states=True)[1]\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_stable_diffusion(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_dir=\"datasets/designs\",\n",
    "    caption_file=\"datasets/caption.txt\",\n",
    "    output_dir=\"fine_tuned_model\",\n",
    "    num_epochs=10,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=4\n",
    "):\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=\"fp16\"\n",
    "    )\n",
    "\n",
    "    device = accelerator.device\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\"\n",
    "        ).to(device)\n",
    "\n",
    "    # Get tokenizer from the pipeline\n",
    "    tokenizer_1 = pipeline.tokenizer\n",
    "    tokenizer_2 = pipeline.tokenizer_2\n",
    "    # if tokenizer is None:\n",
    "    #     # Fallback to default CLIP tokenizer if pipeline tokenizer is not available\n",
    "    #     tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    vae = pipeline.vae.to(device)\n",
    "    unet = pipeline.unet.to(device)\n",
    "    text_encoder_1 = pipeline.text_encoder.to(device)\n",
    "    text_encoder_2 = pipeline.text_encoder_2.to(device)\n",
    "    \n",
    "    # Freeze VAE and text encoder\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder_1.requires_grad_(False)\n",
    "    text_encoder_2.requires_grad_(False)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CustomDataset(image_dir, caption_file, tokenizer_1, tokenizer_2)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        unet.parameters(),\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Prepare for training\n",
    "    unet, optimizer, dataloader = accelerator.prepare(\n",
    "        unet, optimizer, dataloader\n",
    "    )\n",
    "\n",
    "    original_size = (1024, 1024)\n",
    "    target_size = (1024, 1024)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        unet.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            with accelerator.accumulate(unet):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float16)\n",
    "                input_ids_1 = batch[\"input_ids_1\"].to(device)\n",
    "                input_ids_2 = batch[\"input_ids_2\"].to(device)\n",
    "                \n",
    "                # Get latent representation\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Add noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0, pipeline.scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],), device=device\n",
    "                )\n",
    "                noisy_latents = pipeline.scheduler.add_noise(\n",
    "                    latents, noise, timesteps\n",
    "                )\n",
    "\n",
    "                # Create SDXL additional time ids\n",
    "                add_time_ids = torch.tensor([\n",
    "                    original_size + target_size + (0, 0)\n",
    "                ], device=device, dtype=torch.float16)\n",
    "                \n",
    "                # Get text embeddings from both encoders\n",
    "                prompt_embeds = text_encoder_1(input_ids_1, output_hidden_states=True)[0]\n",
    "                pooled_prompt_embeds = text_encoder_2(input_ids_2, , output_hidden_states=True)[1]\n",
    "\n",
    "                # Reshape time embeddings to match expected dimensions\n",
    "                add_time_ids = add_time_ids.repeat(batch_size, 1)\n",
    "\n",
    "                # Add additional conditioning\n",
    "                added_cond_kwargs = {\n",
    "                    \"text_embeds\": pooled_prompt_embeds,\n",
    "                    \"time_ids\": add_time_ids\n",
    "                }\n",
    "\n",
    "                # Predict noise\n",
    "                noise_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    prompt_embeds,\n",
    "                    added_cond_kwargs=added_cond_kwargs\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = torch.nn.functional.mse_loss(\n",
    "                    noise_pred, noise, reduction=\"none\"\n",
    "                ).mean()\n",
    "                \n",
    "                # Backward pass\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.detach().item()\n",
    "        \n",
    "        # Print progress\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            pipeline.save_pretrained(\n",
    "                os.path.join(output_dir, f\"checkpoint-{epoch+1}\")\n",
    "            )\n",
    "    \n",
    "    # Save final model\n",
    "    pipeline.save_pretrained(output_dir)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bb2ffc3-d8bc-48b8-9d34-1b87f07da6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99d1968e1c140bca7c5cf73b62b1d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stable_diffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstabilityai/stable-diffusion-xl-base-1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/designs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/caption.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfine_tuned_sd3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 99\u001b[0m, in \u001b[0;36mtrain_stable_diffusion\u001b[0;34m(model_id, image_dir, caption_file, output_dir, num_epochs, batch_size, learning_rate, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m     93\u001b[0m added_cond_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m: pooled_prompt_embeds,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: add_time_ids\n\u001b[1;32m     96\u001b[0m }\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Predict noise\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m    108\u001b[0m     noise_pred, noise, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m )\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py:1150\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m         emb \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m class_emb\n\u001b[0;32m-> 1150\u001b[0m aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aug_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maddition_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_hint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1154\u001b[0m     aug_emb, hint \u001b[38;5;241m=\u001b[39m aug_emb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py:980\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_aug_embed\u001b[0;34m(self, emb, encoder_hidden_states, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m time_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_time_proj(time_ids\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m    979\u001b[0m time_embeds \u001b[38;5;241m=\u001b[39m time_embeds\u001b[38;5;241m.\u001b[39mreshape((text_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 980\u001b[0m add_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_embeds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m add_embeds \u001b[38;5;241m=\u001b[39m add_embeds\u001b[38;5;241m.\u001b[39mto(emb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    982\u001b[0m aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_embedding(add_embeds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "trained_pipeline = train_stable_diffusion(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_dir=\"datasets/designs\",\n",
    "    caption_file=\"datasets/caption.txt\",\n",
    "    output_dir=\"fine_tuned_sd3\",\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2b75f-3f1a-4ac6-ba12-5ee9bb843420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab0a01-012c-4330-8572-5e440be6f1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
