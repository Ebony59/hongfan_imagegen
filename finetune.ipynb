{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fe97d4-7e02-4456-af46-c2397c38c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline, StableDiffusionXLPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eefd23a-72fb-4de8-9129-122cc4d01825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef79634058114560a9d11acaa3014b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8c289be98741ab824adeaa85e7c394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "# pipe = StableDiffusion3Pipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9cd69d-e9ac-49be-96df-f9124edd4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32, \n",
    "#     lora_dropout=0.1, \n",
    "#     bias=\"none\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed570931-9fdd-4cbe-89a9-d37b6a1223a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b198bddd19f74b18a9235c5dec9c1e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cc36213e2f44658f5fdfe43e60c84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb9fef93c0e4d438cb1d9e0601da0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc9d6c30dec4f7989e6143b11e73be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6db3b16f95404a9086d804f861f4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f43df9d0f470fb970f4f8598b11bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6252ae7f4aa443cbbe63c28ead53a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6520c730-fa2e-45f1-8aa8-20fc6c65c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    caption = model.generate(**inputs)\n",
    "    return processor.decode(caption[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8634a785-740a-427e-9055-3538071c2c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p03.jpg: gnome with a flower and butterfly on a white background\n",
      "p06.jpg: a black and white drawing of a hand with a triangle and all seeing symbols\n",
      "ghouldfriend_p01.png: a close up of a black bat with big eyes and a big nose\n",
      "p02.jpg: cartoon illustration of a bee with a pot of honey\n",
      "p12.png: two wooden nutcrackers with pine cones and hollyconnets on them\n",
      "p10.jpg: there is a snowman with a hat and scarf holding a present\n",
      "p01.jpg: gnome with a butterfly on his head holding a flower pot\n",
      "p11.jpg: a close up of a toy figure of a cat with a cup\n",
      "p09.jpg: there is a snowman with a scarf and hat holding a red ribbon\n",
      "p08.jpg: a close up of a penguin with a christmas present on a calendar\n",
      "p04.jpg: a close up of a halloween candle holder with a witch hat on top\n",
      "p05.jpg: there are three pumpkins stacked on top of each other\n",
      "p07.jpg: a close up of a skeleton hand holding a skull on a stand\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"datasets/designs/\"\n",
    "caption_file = \"datasets/caption.txt\"\n",
    "for img in os.listdir(image_folder):\n",
    "    caption = generate_caption(os.path.join(image_folder, img))\n",
    "    print(f\"{img}: {caption}\")\n",
    "    with open(caption_file, \"a\") as f:\n",
    "        f.write(f'{img}\\t{caption}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3af273-584f-45b7-9992-48e16a272f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_file, tokenizer_1, tokenizer_2, target_size=1024):\n",
    "        self.image_dir = image_dir\n",
    "        self.target_size = target_size\n",
    "        self.tokenizer_1 = tokenizer_1\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        \n",
    "        # Load captions\n",
    "        self.image_caption_pairs = []\n",
    "        with open(caption_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_name, caption = line.strip().split('\\t')\n",
    "                self.image_caption_pairs.append((image_name, caption))\n",
    "\n",
    "    def resize_and_pad(self, image):\n",
    "        \"\"\"Resize image maintaining aspect ratio and pad if necessary.\"\"\"\n",
    "        # Get original dimensions\n",
    "        original_width, original_height = image.size\n",
    "        \n",
    "        # Calculate aspect ratio\n",
    "        aspect_ratio = original_width / original_height\n",
    "        \n",
    "        if aspect_ratio > 1:  # Width > Height\n",
    "            new_width = self.target_size\n",
    "            new_height = int(self.target_size / aspect_ratio)\n",
    "        else:  # Height >= Width\n",
    "            new_height = self.target_size\n",
    "            new_width = int(self.target_size * aspect_ratio)\n",
    "            \n",
    "        # Resize image\n",
    "        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Create new image with padding\n",
    "        new_image = Image.new('RGB', (self.target_size, self.target_size), (0, 0, 0))\n",
    "        \n",
    "        # Calculate padding\n",
    "        left_padding = (self.target_size - new_width) // 2\n",
    "        top_padding = (self.target_size - new_height) // 2\n",
    "        \n",
    "        # Paste resized image onto padded background\n",
    "        new_image.paste(image, (left_padding, top_padding))\n",
    "        \n",
    "        return new_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_caption_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption = self.image_caption_pairs[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB')\n",
    "        \n",
    "        # Resize and pad image\n",
    "        image = self.resize_and_pad(image)\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        image = torch.from_numpy(np.array(image)).float() / 127.5 - 1\n",
    "        image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Tokenize caption with both tokenizers\n",
    "        tokens_1 = self.tokenizer_1(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tokens_2 = self.tokenizer_2(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids_1\": tokens_1.input_ids[0],\n",
    "            \"input_ids_2\": tokens_2.input_ids[0],\n",
    "            \"attention_mask_1\": tokens_1.attention_mask[0],\n",
    "            \"attention_mask_2\": tokens_2.attention_mask[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066f2d1e-6c62-49cf-8295-5b6255732405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stable_diffusion(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_dir=\"datasets/designs\",\n",
    "    caption_file=\"datasets/caption.txt\",\n",
    "    output_dir=\"fine_tuned_model\",\n",
    "    num_epochs=10,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=4,\n",
    "    project_name=\"sdxl-finetuning\",\n",
    "    run_name=None\n",
    "):\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "            \"model\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "            \"image_dir\": image_dir,\n",
    "            \"caption_file\": caption_file\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize accelerator without mixed precision\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=None  # Changed from \"fp16\" to None\n",
    "    )\n",
    "\n",
    "    # Set device\n",
    "    device = accelerator.device\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load base model\n",
    "    print(\"Loading SDXL model...\")\n",
    "    pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        torch_dtype=torch.float32,  # Changed from float16 to float32\n",
    "        use_safetensors=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get components\n",
    "    tokenizer_1 = pipeline.tokenizer\n",
    "    tokenizer_2 = pipeline.tokenizer_2\n",
    "    vae = pipeline.vae.to(device)\n",
    "    unet = pipeline.unet.to(device)\n",
    "    text_encoder_1 = pipeline.text_encoder.to(device)\n",
    "    text_encoder_2 = pipeline.text_encoder_2.to(device)\n",
    "    \n",
    "    # Freeze VAE and text encoders\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder_1.requires_grad_(False)\n",
    "    text_encoder_2.requires_grad_(False)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CustomDataset(image_dir, caption_file, tokenizer_1, tokenizer_2)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        unet.parameters(),\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "    \n",
    "    # Prepare for training\n",
    "    unet, optimizer, dataloader = accelerator.prepare(\n",
    "        unet, optimizer, dataloader\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    flobal_setp = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        unet.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Move batch to device and convert to float32\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float32)\n",
    "                input_ids_1 = batch[\"input_ids_1\"].to(device)\n",
    "                input_ids_2 = batch[\"input_ids_2\"].to(device)\n",
    "                \n",
    "                # Get latent representation\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0, pipeline.scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],), device=device\n",
    "                )\n",
    "                noisy_latents = pipeline.scheduler.add_noise(\n",
    "                    latents, noise, timesteps\n",
    "                )\n",
    "                \n",
    "                # Get text embeddings\n",
    "                with torch.no_grad():\n",
    "                    # First encoder\n",
    "                    text_outputs_1 = text_encoder_1(\n",
    "                        input_ids_1,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    text_hidden_states_1 = text_outputs_1.hidden_states[-2]\n",
    "                    \n",
    "                    # Second encoder\n",
    "                    text_outputs_2 = text_encoder_2(\n",
    "                        input_ids_2,\n",
    "                        output_hidden_states=True,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    text_hidden_states_2 = text_outputs_2.hidden_states[-2]\n",
    "                    pooled_text_embeds_2 = text_outputs_2.text_embeds\n",
    "                \n",
    "                # Ensure proper dtype for embeddings\n",
    "                text_hidden_states_1 = text_hidden_states_1.to(dtype=torch.float32)\n",
    "                text_hidden_states_2 = text_hidden_states_2.to(dtype=torch.float32)\n",
    "                pooled_text_embeds_2 = pooled_text_embeds_2.to(dtype=torch.float32)\n",
    "                \n",
    "                # Concatenate embeddings\n",
    "                prompt_embeds = torch.cat([text_hidden_states_1, text_hidden_states_2], dim=-1)\n",
    "                \n",
    "                # Create time embeddings\n",
    "                add_time_ids = torch.tensor([\n",
    "                    1024, 1024,  # Original Size\n",
    "                    0, 0,        # Crops top-left\n",
    "                    1024, 1024,  # Target Size\n",
    "                ], device=device, dtype=torch.float32)\n",
    "                add_time_ids = add_time_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "                \n",
    "                # Add conditioning\n",
    "                added_cond_kwargs = {\n",
    "                    \"text_embeds\": pooled_text_embeds_2,\n",
    "                    \"time_ids\": add_time_ids\n",
    "                }\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    prompt_embeds,\n",
    "                    added_cond_kwargs=added_cond_kwargs\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = torch.nn.functional.mse_loss(\n",
    "                    noise_pred,\n",
    "                    noise,\n",
    "                    reduction=\"mean\"\n",
    "                )\n",
    "                \n",
    "                # Backward pass\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.detach().item()\n",
    "                \n",
    "                if step % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "                    # Log to wandb\n",
    "                    wandb.log({\n",
    "                        \"loss\": current_loss,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"global_step\": global_step,\n",
    "                    })\n",
    "\n",
    "                global_step += 1\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            pipeline.save_pretrained(\n",
    "                os.path.join(output_dir, f\"checkpoint-{epoch+1}\")\n",
    "            )\n",
    "            wandb.save(os.path.join(checkpoint_dir, \"*\"))\n",
    "    \n",
    "    pipeline.save_pretrained(output_dir)\n",
    "    wandb.save(os.path.join(checkpoint_dir, \"*\"))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb2ffc3-d8bc-48b8-9d34-1b87f07da6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading SDXL model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ec0073783a4c2da48323e11120be02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Step 0, Loss: 0.5764\n",
      "Epoch 1/2, Step 10, Loss: 0.1260\n",
      "Epoch 1/2 completed. Average Loss: 0.5182\n",
      "Epoch 2/2, Step 0, Loss: 0.0961\n",
      "Epoch 2/2, Step 10, Loss: 0.3688\n",
      "Epoch 2/2 completed. Average Loss: 0.2821\n"
     ]
    }
   ],
   "source": [
    "trained_pipeline = train_stable_diffusion(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_dir=\"datasets/designs\",\n",
    "    caption_file=\"datasets/caption.txt\",\n",
    "    output_dir=\"fine_tuned_sd3\",\n",
    "    num_epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2b75f-3f1a-4ac6-ba12-5ee9bb843420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab0a01-012c-4330-8572-5e440be6f1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
