{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98fe97d4-7e02-4456-af46-c2397c38c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline, StableDiffusionXLPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eefd23a-72fb-4de8-9129-122cc4d01825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef79634058114560a9d11acaa3014b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8c289be98741ab824adeaa85e7c394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9cd69d-e9ac-49be-96df-f9124edd4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed570931-9fdd-4cbe-89a9-d37b6a1223a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b198bddd19f74b18a9235c5dec9c1e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cc36213e2f44658f5fdfe43e60c84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb9fef93c0e4d438cb1d9e0601da0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc9d6c30dec4f7989e6143b11e73be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6db3b16f95404a9086d804f861f4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f43df9d0f470fb970f4f8598b11bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6252ae7f4aa443cbbe63c28ead53a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6520c730-fa2e-45f1-8aa8-20fc6c65c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    caption = model.generate(**inputs)\n",
    "    return processor.decode(caption[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8634a785-740a-427e-9055-3538071c2c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p03.jpg: gnome with a flower and butterfly on a white background\n",
      "p06.jpg: a black and white drawing of a hand with a triangle and all seeing symbols\n",
      "ghouldfriend_p01.png: a close up of a black bat with big eyes and a big nose\n",
      "p02.jpg: cartoon illustration of a bee with a pot of honey\n",
      "p12.png: two wooden nutcrackers with pine cones and hollyconnets on them\n",
      "p10.jpg: there is a snowman with a hat and scarf holding a present\n",
      "p01.jpg: gnome with a butterfly on his head holding a flower pot\n",
      "p11.jpg: a close up of a toy figure of a cat with a cup\n",
      "p09.jpg: there is a snowman with a scarf and hat holding a red ribbon\n",
      "p08.jpg: a close up of a penguin with a christmas present on a calendar\n",
      "p04.jpg: a close up of a halloween candle holder with a witch hat on top\n",
      "p05.jpg: there are three pumpkins stacked on top of each other\n",
      "p07.jpg: a close up of a skeleton hand holding a skull on a stand\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"datasets/designs/\"\n",
    "caption_file = \"datasets/caption.txt\"\n",
    "for img in os.listdir(image_folder):\n",
    "    caption = generate_caption(os.path.join(image_folder, img))\n",
    "    print(f\"{img}: {caption}\")\n",
    "    with open(caption_file, \"a\") as f:\n",
    "        f.write(f'{img}\\t{caption}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3af273-584f-45b7-9992-48e16a272f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_file, tokenizer, image_size=1024):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_size = image_size\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Load captions\n",
    "        self.image_caption_pairs = []\n",
    "        with open(caption_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_name, caption = line.strip().split('\\t')\n",
    "                self.image_caption_pairs.append((image_name, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_caption_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption = self.image_caption_pairs[idx]\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(os.path.join(self.image_dir, image_name)).convert('RGB')\n",
    "        image = image.resize((self.image_size, self.image_size))\n",
    "        image = torch.from_numpy(np.array(image)).float() / 127.5 - 1\n",
    "        image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Tokenize caption\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": tokens.input_ids[0],\n",
    "            \"attention_mask\": tokens.attention_mask[0]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066f2d1e-6c62-49cf-8295-5b6255732405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stable_diffusion(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_dir=\"datasets/designs\",\n",
    "    caption_file=\"datasets/caption.txt\",\n",
    "    output_dir=\"fine_tuned_model\",\n",
    "    num_epochs=10,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=4\n",
    "):\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=\"fp16\"\n",
    "    )\n",
    "\n",
    "    pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    "    )\n",
    "\n",
    "    # Get tokenizer from the pipeline\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    if tokenizer is None:\n",
    "        # Fallback to default CLIP tokenizer if pipeline tokenizer is not available\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    vae = pipeline.vae\n",
    "    unet = pipeline.unet\n",
    "    text_encoder = pipeline.text_encoder\n",
    "    \n",
    "    # Freeze VAE and text encoder\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CustomDataset(image_dir, caption_file, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        unet.parameters(),\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Prepare for training\n",
    "    unet, optimizer, dataloader = accelerator.prepare(\n",
    "        unet, optimizer, dataloader\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        unet.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Get latent representation\n",
    "                latents = vae.encode(\n",
    "                    batch[\"pixel_values\"].to(dtype=vae.dtype)\n",
    "                ).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0, pipeline.scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],), device=latents.device\n",
    "                )\n",
    "                noisy_latents = pipeline.scheduler.add_noise(\n",
    "                    latents, noise, timesteps\n",
    "                )\n",
    "                \n",
    "                # Get text embeddings\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = torch.nn.functional.mse_loss(\n",
    "                    noise_pred, noise, reduction=\"none\"\n",
    "                ).mean()\n",
    "                \n",
    "                # Backward pass\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.detach().item()\n",
    "        \n",
    "        # Print progress\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            pipeline.save_pretrained(\n",
    "                os.path.join(output_dir, f\"checkpoint-{epoch+1}\")\n",
    "            )\n",
    "    \n",
    "    # Save final model\n",
    "    pipeline.save_pretrained(output_dir)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2ffc3-d8bc-48b8-9d34-1b87f07da6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4571d94eec44a55b89733886938bb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a8fb40fcf04a45b3bda42b4bc19635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b996a1a6d684443e96fab62382c3eb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c5b550945e4dd2afc0a382376e4e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/1.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a6ab25484341fe9c7058b50fff63bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44803b17512f404e91ccf9de714b33c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/5.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_pipeline = train_stable_diffusion(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_dir=\"datasets/designs\",\n",
    "    caption_file=\"datasets/caption.txt\",\n",
    "    output_dir=\"fine_tuned_sd3\",\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2b75f-3f1a-4ac6-ba12-5ee9bb843420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab0a01-012c-4330-8572-5e440be6f1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
